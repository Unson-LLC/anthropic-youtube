# Evaluate prompts in the Anthropic Console

URL: https://www.youtube.com/watch?v=KIGBsQqZcNA
数値: 23
時間: 3m

## 要点まとめ

- **(00:00)** Anthropic Workbench に最近加えられた改善により、Claude 向けのプロンプト作成〜デプロイがしやすくなった。プロンプトジェネレータ（prompt generator）が高レベルのタスク記述を、Claude 3.5 Sonnet を用いて詳細なテンプレートに変換する。
- **(00:20)** 例として「カスタマーサポートのリクエストを振り分ける（triage）」というタスクを想定。まずジェネレータがタスクを基に具体的なプロンプトを生成する。しかし、それを本番投入する前に、現実的なデータでテストすることが重要。
- **(00:40)** プロンプトに対する「現実的な入力データ（リアルな customer support request）」を自動生成できる機能があり、これによりテストデータ作成の手間が軽減されている。
- **(01:02)** プロンプトが「正当な理由付け（justification）」と triage の決定を返しており、初見では機能しているように見える。ただし、この結果が偶然かどうかを判断するために「より幅広いシナリオ」でのテストが必要。
- **(01:20)** 新しい “Evaluate” 機能を使えば、多くのテストケースを設定でき、CSV からのアップロードも可能。テストケース生成ロジックは既存のテストセットに合わせてカスタマイズ可能。具体要件があればロジックを直接編集できる。
- **(01:43)** テストケースを十分に用意した後、新しいテストスイートでプロンプトを実行して結果を得る。現時点ではその結果は良さそう。
- **(02:11)** ただし、評価（grading）の段階で「justification（理由付け）」がやや簡潔すぎるというフィードバックが出た。例えば一文だけではなく、二文にしたいという調整。
- **(02:36)** プロンプトを修正（例：justification を一文 → 二文へ）すると、新しいプロンプトを既存のテストセット全体に対して再実行できる。新旧の出力を比較できる。
- **(03:06)** 出力の triage 判定自体は大きな変化なし。しかし、評価スコア（grading）は平均して改善されている。プロンプトの質を改善するためのフィードバックループが機能していることを示している。

Transcript: "(00:00) We've recently made a number of improvements to the Anthropic Workbench that make it easier to develop and deploy high-quality prompts for Claude. Let's see how it works by taking a look at our recently updated prompt generator. You can use the prompt generator to take a high-level description of a task and convert it into a detailed prompt template using Claude 3.5 Sonnet. (00:20) In this case, let's imagine we need to triage customer support requests. As you can see, Claude immediately starts writing a prompt based off of our task. It's detailed and specific and looks like it should work. But, before we deploy it to production, we should really test to see how it performs with realistic customer data. (00:40) Coming up with realistic test data can be time-consuming and it can take longer than writing the prompt itself. You can now use Claude to automatically generate realistic input data based off of your prompt. In this case, we can generate a customer support request. This one looks good, so let's see how the prompt works with this particular support request This seems pretty good. (01:02) It's providing a justification and a triage decision. But how do we know that we didn't get lucky? How do we know that this prompt is actually going to work in a broad range of scenarios? That's where the new Evaluate feature comes in. You can use the Evaluate page to set up as many test cases as you want. (01:20) Let's keep generating a broad range of representative test cases. You can also upload test cases from a CSV if you happen to have the test data in it. Test case generation logic is highly customizable and adapts to your existing test set. If you have highly specific requirements, you can directly edit the generation logic yourself. (01:43) Once you have enough test cases ready, you can generate results for your new test suite. Alright, these results look pretty good, so let's go and grade their quality. Maybe we decide when we're evaluating them that we actually felt that the justifications were a little brief. We'd like them to be a bit longer. (02:11) Well, we can go back to the prompt and find the section where it specified a one sentence justification and update it to a two sentence justification. We can rerun the prompt, and just as we'd hope, we're seeing a two sentence justification. So let's go back to the evaluate tab, and thankfully, our test suite is still there. (02:36) So it can rerun the new prompt against the old test set data. And just as we hoped, they're all just a little bit longer. We can go and grade these new outputs. We're happier with these ones. But just to be sure, we can actually compare these new results against the old results. And here we can see, side by side, the results are longer. (03:06) We're still getting similar triage decisions, but our grading, on average, is better.”