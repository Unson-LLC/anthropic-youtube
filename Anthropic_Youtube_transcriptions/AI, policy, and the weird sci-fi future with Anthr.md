# AI, policy, and the weird sci-fi future with Anthropic’s Jack Clark

URL: https://www.youtube.com/watch?v=b1-OuHWu88Y
数値: 28
時間: 38m

- Hello and welcome to another conversation from Anthropic. My name's Stuart Ritchie. It's my job to talk to our researchers and our other staff about what they're thinking about AI today. I'm here with Jack Clark, who's one of our co-founders and also our head of policy.
- And you might know him from his newsletter ImportAI, which is a really useful and opinionated weekly summary of what's been happening in AI. So thanks very much for doing this Jack. Before we start, can you just tell us a bit about your background and kind of how you got to where you are now? - Yeah, I have a, a bit of a strange background and I took a very tangled path to get here.
- I started out life as a kind of technical journalist, and I treated journalism a bit like method acting. So anything I wrote about, I tried to understand, right? So when I wrote about databases, I taught myself SQL. When I wrote about like computer chips, I took, learned about semiconductor manufacturing. - Nice.
- And at some point I became obsessed with data centers and I started this series called the Clark Side of the Cloud. I used to tour data centers around Europe and the rest of the world and get really into it. And I remember at some point in about 2010, I realized that someone like Google was gonna use machine learning on all of these data centers, and they built all of these computers and were gonna do something with it.
- So I moved to America to cover what was then the very nascent field of AI called myself the world's only neural network reporter, which was easy to do in 2012. And then I joined OpenAI in 2016 and very quickly became obsessed with policy because I realized how important this was going to become and how little people in policy knew about it.
- I've been working on that ever since. - Right. And now you're the, you know, our Head of Policy, as I mentioned, we're in London today. Yesterday you were at the Labor Party conference. For any international people, Labor are the current governing party of the UK.
- And you were talking at their conference, they've just come into government, presumably they have a million people telling them that, that this is the priority, this is the priority, this is the priority. What was, you know, they had an AI event, so obviously they're thinking about AI to some extent. What's your impression of how they're thinking about ai? To what extent do they care about this stuff? - Yeah, it's been really striking to see them constantly mention AI in their bucket of technologies or things that are going to help grow the British economy.
- So the Labor Party, as those who are here know is, is kind of obsessed with how we get Britain growing again. And they've identified AI as one of the key things to do. They've also inherited this thing called the AI Safety Institute, which was started by the conservative government and is like a national asset, you know, and Anthropic has done pre-deployment testing with the AISI.
- So what was striking to me as the UK government is currently thinking about how to kind of build on the legacy of its political predecessors, which is unusual. Usually you want to come in and like rip all of this stuff up, but they've actually come in, are aware of how valuable the AISI is and they're trying to think about how to, how to make more of it.
- And can I just be, just be clear, the AI Safety Institute, AISI is pronounced AISI. - Yeah. - And that's opposed, as opposed to the one in the US. - Yes. - Which is called AC in Washington, or- - USAC or ACDC. - Yeah. - I was, I was leading you- - Which is the unofficial name that you know that no part of the US government has sanctioned. - Right. Yes, exactly.
- So, so, and then, you know, our previous Prime Minister Rishi Sunak talks a lot about AI safety. That was one of his big things. They've inherited the Labor Party, have inherited the AI Safety Institute. To what extent are they worried about AI safety as opposed to AI as a tool to help grow the economy? - So in my conversations, they, they care about AI safety in so far as they care about protecting the public from things that could, you know, cause, cause harm to kind of life and limb was a phrase used by one of the MPs I was speaking to. So in this, they're going to be similar
- to the US government and others who focus on kind of catastrophic risks like bio or cyber. But beyond that, my general sense is they want to deal with some aspect of safety, but also just think about how to utilize AI and how to get the kind of British government working better using AI technology as well.
- And do you, do you consider it that they're thinking of the safety risks in the way that you might think about the risks of social media or, or, or other technologies that have come before rather than these kind of novel and, and potentially much scarier risks that we often talk about when it comes to generative AI? - Yeah, I mean, they're two months in. I think there'll be some period of, of education right now. I think they're being inundated with information about this.
- And I didn't choose to make yesterday my, let's open the briefcase on all of the scariest things we can imagine talk. I was mostly like, let's talk about how to make sure that the AISI like continues to thrive. Because we think that that's ultimately useful for safety, and then we're going to get to some of the weirder risks in a while. - Right, right. Makes sense.
- When it comes to the economic aspect of this, you know, as you said, the Labor Party have talked about growth constantly since they were, since they were elected. We're seeing pieces coming out, talking about the foundations of the UK economy needing, you know, fundamental fixes. Do you see AI yourself as a big part of that? I mean, what, what's the sort of picture of how AI could help boost the UK and indeed other economies? - Well, you know, when we deploy AI systems today, one of the main things we see is that businesses have a load of paper plumbing inside them. Most businesses are actually like collections of processes
- that like help you grow from something like a customer talking to you to some action, like selling them something or dealing with a complaint. And a lot of where language models and kind of these powerful systems are being used is in that inner kind of glue of a business. Governments are giant bureaucracies full of paper.
- And so I think actually a lot of what people are most excited about is the kind of back office aspects. How do we get things like, you know, the paper that's produced in the NHS to sort of flow more effectively through the system? How do we handle constituency responses for MPs who are inundated with their constituents needs and need a better system for kind of filtering and analyzing them? And generally AI, you know, holds this promise of taking a load of the stuff that is around us and making it kind of be something that we can actually handle appropriately as, as humans and pay proper attention to by using these generative systems to kind of read and classify and understand this kind of mountain
- of paper around us. - You've said that, and this really struck me when you made this point, that even if we don't get the sort of progress in AI, that we often talk about scaling laws, the models becoming more intelligent over time, we'd still find dramatic economic and maybe other social benefits in the AI models that we have right now.
- So even if somehow the government banned us from making, you know, the next generation of AI models, we'd still get value out of it. How are you thinking about that? - I think of it a bit as like, we've just sort of discovered electricity and we've put the first light bulbs in the factories, and you could stop all electricity kind of efficiency refinement from there and just have this basic new thing.
- And then you'd still build factories around the assumption of electricity existing. So rather than hanging light bulbs in them, you'd be building production lines on the assumption they had access to something like electricity or something like this. So AI, it just has a huge way to go in terms of being integrated into the economy and, and sort of building new and exciting businesses.
- And if we stopped everything today, some teenager somewhere is going to find a completely mind blowing use for Claude or any of these other systems that we've never anticipated. And there'll be tens to hundreds of those in our future. - Right? Yeah. We've got all these developers out there that are working on, you know, using Claude and coming up with all these amazing ideas. - Yeah. That, that, that makes sense. But of course, it's not stopping.
- We, we, there's, there's no indication as far as I'm aware that this is stopping or, or, or, or, or even really slowing down. And of course, you know, a lot of people talk about AI as just being, you know, people talk about "stochastic parrots", so they're just repeating things that they've previously learned, but in a kind of jumbled up random way or just being these kind of inert, you know, next token predictors.
- They're just not, there's nothing interesting going on in there. But one of the things that you talk about quite a lot in your newsletter and elsewhere is that these are, these are much more than that. These, these models are much more than that. I'm just gonna read a, a quote from your, from your blog a few weeks ago. That AIs are creative mirrors machine spirits of the human unconscious value simulacra.
- We're not dealing with, here, we're not dealing with simple tools. We're dealing with vast, high dimensional artifacts that encode within themselves the culture on which they've been trained and can reflect this culture back. Can you, can you talk a little bit about, you know, it, it might sound quite boring, it might sound like some new piece of sales software if we're talking about how it might boost the economy.
- But when you put it in the terms that I've just used, it's much bigger than that. - Yeah. I mean, a hammer doesn't have any instincts on which nail it wants to hit. And these AI systems we've built have a kind of artificial intuition, and that's really spooky and strange. We've never really built tools before that understands something of like the human world and have some of those instincts inherited from it.
- So with these AI systems, they, they have values within them. They have some level of, of creativity. And when we look at the sorts of like patterns they uncover or the insights they have, they display what many, many, many kind of reasonable people would de describe as like creativity or intuition.
- Now, sometimes it's not very good, right? But we still have like a hammer that's being creative, which is wild. Yeah. And it's something that is very, very unusual. And so the frame with which I'm trying to sort of talk to policy makers is this isn't like a technology. This is much more, and I, I said this to the UN Security Council last year, and I, I've kind of been expanding this, this idea recently.
- It's much more like we figured out a way to simulate some aspect of people and to extent some aspect of like how countries work. And it's like these AI systems are like these, these silicon countries which we're importing into the world of all of these incredible capabilities. And that's never happened before. - Let's stick on that metaphor of silicon countries.
- Because you've got a, a way of thinking about AIs as the "rogue state" theory of AIs. And this is going beyond, you know, the, the effects in one particular country. And it's the way of thinking about how Ais, you know, will, will, will work in, in the world and how governments should think about AIs. Can you talk us through the "rogue state" theory of AI? Yeah.
- So this idea, I've been talking to governments about AI for a long time, and I found myself in these different conversations first about self-driving cars and then about computer vision capabilities, and then about what does AlphaGo mean? Or like reinforcement learning systems and now language models. But if you look around the world over that time, it hasn't been the individual technologies that have mattered.
- It's been the arrival of this kind of utility-scale type of computing that's, that's mattered. And it's also the emergence of new systems that everyone needs to kind of study and reckon with in a much more holistic way than like a, a single part of government.
- And so I've started to say to governments, you should think of AI systems as kind of like countries that are arriving into the world and misaligned AI systems as like rogue states. So, you know, the, the, the reason why this analogy feels helpful is that when you talk to governments about safety, you might talk about bioweapons or you might talk about cyber risk, or you might talk about, you know, phishing.
- Well, all of those have different parts of government set up to respond to it. But if you talk to government and say, AI systems are like a new country that's doing bad stuff that you do not understand, it requires them to think much more holistically about how they deal with that. And it means that you can say to them, you need a whole of government response to AI systems, which actually, like, sounds a lot more sensible if you think of it like a, like a country instead of a technology.
- And, and, and there's other, there's other analogies too. I mean, we spend a lot of time in Anthropic working on interpretability. Yeah. So we're trying to look into the models and work out how they work because nobody actually knows how they work. Nobody can tell you exactly why Claude or ChatGPT4 or any of these models actually produces the, the, the response that they do.
- And so there's a kind of, the equivalent there is kind of Kremlinology, right? It's kind of like trying to guess what's going on inside and, and, and, and work out in order to try and have some level of predictability, I guess. - In the same way that our interpretability team is trying to work out, just like how do language models work? What are the systems by which language models make decisions? How do we take like input information into a language model and look at all of the internal deliberation that goes on and, and look at an output? Well, the CIA does the same thing about North Korea or like Iran. - Right, right, right.
- In some sense we're, we're grappling with a similar class of problem. AI systems are opaque and we desperately want to understand them because they have immense value and also some potential for risk. Countries are opaque and you spend a lot of time understanding them and their potential for risk. And rogue states are opaque and are risky. And, and we actually like respond to these things in similar ways.
- I get, I mean, I mean, I wonder if that analogy might make us, like how confident should that analogy make us, because I, I can't think of that many countries that we would consider as rogue states that are now completely aligned. Right. There's maybe a handful of countries that are considered, you know, unambiguously rogue states.
- I'm not sure how well we've done, like, as an international community to bring them back into the fold. - Well, you know, we had the, the kind of disintegration of, of Russia at the end of the Cold War. And we had a lot of Eastern European countries, which have now been somewhat integrated into the broader economy.
- They changed their value systems, they gained institutions, they gained the ability to integrate into our world and kind of trade within it. And even to some extent this happened with Russia, but obviously now where that, that pendulum is- - It's gone back in the opposite direction. Yeah, yeah. - Yeah. But I think that it also, I hope points to a kind of inherent optimism here, which is that in some way, AI systems may be easier to deal with than countries because countries kind of run on on human time and have fewer points of intervention. AI systems run on machine time and have more points of intervention.
- This is both like a challenge from a safety perspective, but also it makes me think that we can build kind of technocratic means of understanding these systems and also understanding how we can trust them and how we can develop confidence in them. And the technology is evolving faster than the kind of countries in the world around us. So I think we can bring them into the fold sooner rather than later.
- Let me pick up on something that you just mentioned. You talked about machine time being different from human time. Now machine time is something that you've, you've talked about recently because you, you, you said it again, I'll quote from what you wrote in ImportAI, the best argument for AI risk is about the speed of human thought versus the speed of machine thought.
- So there's a recent paper from Caltech researchers where they point out, so humans think really slowly compared to the rate at which we take in information from the world, humans think about 10 bits per second, whereas our sensory inputs are about one gigabyte per second. Future AIs are gonna think extremely fast.
- Can you sketch out why, you know, what the kind of threat model is for why this makes them such a threat? - I mean, even in our, our world around us, it's, it's really difficult to catch a fly or a mosquito. They're faster than you. They're, they're more agile. They operate on like a faster clock rate than you, I've never tried to catch a hummingbird, but similarly - Very difficult, I assume. - Yeah, yeah, yeah, yeah. Less, less likely. Something you want to catch also. - Yeah, yeah. - But I think that there's something really persuasive about this
- argument, and as someone who I think came to the notion of AI safety later than some of our colleagues at Anthropic and almost plays the role of like an internal skeptic about some of these ideas. I found this to be a really convincingly useful frame because it, it, it's the same problem we encounter in policy.
- Like how are policymakers meant to respond to a technology evolving this rapidly? Well, how are people meant to respond to a point technology that's moving like much, much faster than them? It's really, really, really challenging. You know, if you are, if you are walking around and you were trying to solve the problems created by cars and you couldn't move as fast as them, it'd be quite challenging. Really. - Indeed. - It's, it's also just maybe just to expand this a bit.
- Like, one of the, one of the things that we see in military conflict today is come countries spend a huge amount of time thinking about their cycle time. Their so-called OODA loop, you know, the observe, orient, decide, act loop. It's all about helping their individual soldiers, groups of soldiers, things like artillery, things like air response move faster. And whoever has a faster OODA loop tends to win.
- And that's just humans competing with one another within the same magnitude of time. So why would we expect to be successful in some kind of conflict with a machine system moving 10 times faster than us when the history of like human military doctrine says, you pretty much always lose. - It's, it, I think it's, it's a really good way of thinking about it.
- 'cause if you, if you say to people, you know, this is gonna be much more intelligent than the most intelligent human, I think that's quite hard to to picture. Yeah. Whereas if you, if you, if you think, well, you know, it's faster. Yeah. How quickly does, how quickly does Claude produce text? Yeah, yeah. Like, look at how quickly that happens. Imagine if it's performing actions or some sort of agentic actions at, at, at, at, at vastly quicker speeds than, than, than we could, I suppose. I mean, given, you know, to the extent to which intelligence is based on speed,
- I guess we're talking about the same thing here. So, you know, that's just one way of being really intelligent to, is to be able to do things really quickly. So, I mean, what's the response to this? Do we set a speed limit for how fast AI can, can, can, can work? I mean, what's the kind of the answer to this other than, you know, trying to align these very fast thinkers to our values.
- I mean, some of what we're doing at Anthropic with the responsible scaling policy and other approaches for the product, I think holds a lesson. Like we can't at human speed classify everything, but our trust and safety filters pick up. But we can train language model-based classifiers to look at those and tie into an enforcement process.
- So what are we doing here? We're training kind of very specific purpose machines to intervene against the other fast moving machine when it does something that's kind of off base. So there's definitely a part of this, which involves building a load of specific kind of AI tooling to further improve kind of safety.
- We're also going to, as you said, need to arrive at some notion of what appropriate interface speeds look like. This could be something like the rate at which AI systems can take actions. It could be the rate at which they can generate text. I mean, at the really basic level what could just be the rate at which we allow the kind of API of an, of a semi-independent AI agent to take in information and output information where you can put some artificial limiter on that.
- None of these are like silver bullets, but they all get at the problem, which is you're trying to constrain this thing that moves faster than you into your kind of subjective universe, which is all, by the way, everything we're talking about is really weird. I think we started the conversation and I was like, oh, yes, these things are like gonna help with like backend bureaucracy, which is true.
- Yeah. But we're talking about very fast moving machine intelligences - Yeah. Yeah. - That have all of these weird properties - Yeah. That have all these abilities to potentially do things like develop motivations and things that we didn't expect all at lightspeed. Yeah. You know, behind our, behind our backs.
- And that's, but it's also great at summarization and coding, right? - Yeah, - Exactly. Both of these are true, which is just a wildly confusing part about the problem. - Totally, totally. It's a very weird thing. Let's talk about one weird aspect of, of, of the, the world of AI actually.
- Because, you know, we have, we have our safety procedures that we, we work on, we have our safety researchers and so on, which you can hear a lot about on our website. And you know, we have other, you know, a whole bunch of stuff online for people to read about this. But there's also a whole world out there of sort of strange community of often anonymous researchers who are kind of pushing these ais to their limits.
- Sometimes they're doing things like getting the ais to talk to each other and having all these very strange conversations. Sometimes they're trying to jailbreak the models to, you know, see really what they can do without the safety procedures that are put in place you often interact with Yeah. These kind of anonymous people.
- How, how, how's that? - And they have great names like Janus and Pliny the Prompter. - Yeah. Yeah. - I think the most cyberpunk thing about the time we're living in is there are semi-anonymous people online who have actually talked to some of these AI systems for thousands of hours, possibly more than almost anyone who works at any of the labs.
- Even though we have some people that, that love talking to Claude, you have people outside that have really specialized in this. And I think that what we're seeing is, is, is kind of science through art. Like I think some of this, some of this stuff is, is science that we can use known techniques for. Some of it looks more like kind of play and theater and psychology all wrapped into one being done by, by these people who kind of have a vision and, and are slightly like off consensus. And I think that when I, when I look at
- that experimentation, it, it to me is some of the most convincing evidence that we're dealing with truly strange technology. And I'm not gonna be able to make strong claims about the different personalities that like Claude or Gemini or ChatGPT have.
- It's hard to know how to evaluate them, but you can look at the work of these people and you clearly see differences. You have to reconcile these things. So I view these as like pointers to science and, and larger amounts of science that other groups will do. And these are kind of the explorers on the, on the frontier. - Right. Right, right.
- And, and it, they might be seeing some of the results of our character training for Claude. - Whenever a new Claude comes up, they play with it and they're like, oh, its personality has changed in this way. And, you know, you, you and I maybe need to be careful and put all of this stuff in air quotes, but to them it's just a completely natural way of speaking about the systems because they've got their own, you know, their own exploratory method. - Right.
- They're, they're really living in a sci-fi novel where people talk to droids every day or whatever it is, and they're having conversations with them - And they can have a business card that says, like Jack Clark, machine psychologist. - Right. Exactly. Yeah. Great name. Yeah. It's, it's remarkable. And actually talking about, you know, science through art and science fiction and so on. This is what you do every week in your newsletter, right.
- You write at the end of the newsletter "tech tales" you call them, which are sometimes long, sometimes short stories, creative writing about the weird future of AI. Can you talk about why you decided to do that? Yeah. And like your process of how you think about these things. - So I'll make a very obscure reference here, but it'll in a roundabout way get us there.
- There's this band called Jawbreaker. Do you know of them? - I've heard of them, I couldn't tell you any of their songs. I'm aware of the, the, the, the band exists. - Yeah. They have a song called Accident Prone, depressing song I think about alcoholism, but it has has a, has a phrase in it, which has always stayed of me, which is, my fiction beats the hell out of my truth.
- And it's something about how the, the stories we tell are often like true of, and how we might just factually describe the things we experience. And I write these stories because I'm trying to sort of reckon with the AI stuff happening around us by imagining situations involving it. And a lot of the stories are based on specific technologies. I also think that the stories probably hold more truth about what I am feeling working at these AI labs from a newsletter themselves.
- And so something I've started doing, which is wonderfully recursive, is I feed all these stories into Claude and I ask it to guess questions about the author writing the stories. And it's, it's really, really strange. Claude, successively more advanced versions of Claude have started to really nail my personality by reading my fiction.
- And also I ask Claude like, what's going on at the lab? And sometimes it's like told stories about, none of which is written in the stories, but which have been unnervingly true to things I've experienced at Anthropic. - Right. Inferring it from the mood of what you've written. Yeah. - And, and, and also just, you know, I think, I think that it's also a way to try and reckon with the, the actual strangeness.
- Like it's not really appropriate in a policy context to say we're dealing with like an alien mind here that's like looking at us. - Right. Yeah. - But I can write a short story about that. Yeah. And it goes, it goes to the same inboxes as for factual stuff. So I'm sort of like, here's your like serving of weird that I've smuggled in at the end. - Yeah. I, I, and you know, I saw that at least one of the stories within a few weeks actually did become real.
- Yeah. You had the story about this AI sort of collapsing into this strange amnesia. And then a company that had a, that had a model report described exactly the thing that you were talking about. And then- - Yeah.
- Yeah. I think that was Nous Research and they had a new model and at a certain parameter point it would start to display something that looked a bit like situational awareness and discomfort. And it, I, I'd theorized this in a story I wrote called the Id Point, very strange. - Right? Yeah. - Yeah, yeah. One, one other thing is I wrote a story last year called, called Replay Grief about a guy talking to, to his, his wife.
- But through the course of the story, it turns out he's not really talking to his wife. He's talking to a language model simulating her after she died. - Right. - Sad story. - Yeah. - I'm, I'm a cheerful guy. I guess I get my sadness out this way. But a few months later there was an op-ed in the New York Times about a woman whose partner had died and she'd fed all of his writings into a language model and was talking to him.
- And, and I, and I just found this really spooky, uncanny all of his stuff is like happening in the world too. - Oh yeah. Yeah. Totally. And, and, and you know, the, some other things which you talk about where you talk about things like the ai, the automated AI scientist that generates hundreds of scientific papers and a a a phrase that struck me as you talked about these new developments being written by the joyfully insane. - Yeah.
- So it's like just the, the world has become a vastly stranger place. Yeah. And we're just talking about it as if it's just normal. Oh, by the way, I've got an automated scientist that can do hundreds of papers a week now. - Yeah. Or, or there was the kid in San Francisco recently who, he was a University of Waterloo graduate, and he decided to build a nuclear fusor. He'd never done hardware stuff in his life.
- He did it using Claude. - Using Claude. I saw that, yeah. - About two weeks, very matter of fact. And he's like, yeah, this AI, this like, you know, AI brain in the sky helped me build like a nuclear fusor of it's in my bedroom, which is crazy. It's like crazy stuff that's happening. = Totally. And it's, it's happening right in front of our eyes. Yeah.
- And I suspect more of it will, you know, more and more and more of this is gonna happen as, as the models become smarter and they start being used for, you know, more, more, more purposes. - I, I think there's an important point to, to kind of linger here on about policy though, which is, and, and it speaks both to the, the value of this technology and some of the risks, which is, you know, the blocker on human flourishing so often is, is is access to education or access to advisors, access to people with time.
- These AI systems are, are truly useful like didactic engines. And that, that's a lot of the uses we see. We see people using them to kind of answer mundane questions, using them to help them with basic things, using them to educate themselves, using them to study scientific papers, using them to learn languages, you, you name it.
- And when I talk to kind of policy makers, I'm trying to impress on them that this is like an amazing social utility in the same way that, you know, YouTube means there's huge amounts of educational content available online or Khan Academy or what have you. It's also where some of the kind of risks that we reckon with kind of come from like frequently risky things don't happen in the world because the number of people that wanted to do the bad thing were small in number and had access to almost no knowledgeable advisors.
- And it's one of the things that AI changes. And so, you know, sometimes I think people talk about Anthropic as though were, you know, doomers or, or what have you. But the position I always hold is that the, if we want to get all of the benefits of this technology, we need to reckon with the fact that it can provide like differential acceleration to bad people as well as good people. And that challenge is just innately hard to deal with.
- But we can't, we can't just ignore it because, because the models are getting better and better and better and there continue to be some number of insane people in the world that want to cause harm. We need to reckon with this intersection. - There's a kind of a, a a weird dynamic where you almost feel as if the people who are saying that there's no risk in these technologies almost don't really believe that they're that powerful. Yeah.
- They, they, they almost don't believe that there could be loads of good things as well because you have, if you accept that there are good things there, there have to be bad effects with these models too. Right. Yeah. So you're almost like denying the, the power of these models. - The, the best take I heard on this recently was someone who noted that today's like accelerationist are actually technological pessimists. Right? - Right. - Yeah. Because they think it just accelerates a little further from where it is today and then stops.
- I think if you are a true accelerationist, you kind of reckon with shock and awe and some small amount of dread of the implications of what happens if this stuff keeps, get betting keeps sort of getting better and better. And another story I sort of tell policy makers is like, look, if we're wrong, and as you said at the start of a conversation, if this technology kind of hits a wall and we just stop it today, great.
- We're gonna get loads and loads of benefits and some probably small amount of risk and, and we'll manage if we're right, we're going to need kind of new institutions, new systems of government, and we're going to need to reckon with both vast abundance and the potential of vast fret. So I guess let's hope that we're right. Yeah. Or hope that we're wrong.
- That part still feels like the, one of the, one of the kind of least clear aspects of this. - Yeah, absolutely. Let's return to a policy question in the upcoming year, we've got various summits coming up. Governments are kind of grappling with AI in different ways. Some are more worried than others about the safety stuff. Some are more convinced that this could be an economic benefit and so on.
- You're, you know, very in touch with the, the, the, the policy world. What do you think is coming up in the next year or so? - So it's going to be a, a really busy, busy year. And also let's not forget that us and the other labs are gonna produce better AI systems during the coming year. It's kind of assumed, but it's worth stating totally that the systems will get better. We have the continuation of the safety summits.
- There was Bletchley Park in 2023, then there was Seoul in, in Korea this year. There will be the French summit coming up in February next year. That's where countries are gonna convene to think about safety and AI systems and coordination about them. There is the small matter of the presidential election in the US - Right? Yep. Yep. - So we're gonna- - Policy concern. Yeah. I guess - Yeah.
- The small policy differences between the two candidates currently. - Yep, yep. - And what's gonna, what that's going to lead to is a new administration. Every administration thinks about getting stuff done in the first 100 days.
- So January, February, March, we can expect whichever administration is in place to make some moves on AI, which could be quite impactful. And finally, we have the European Union has the AI Act, which is coming into force and is gonna go into kind of implementation mode next year. So that means AI companies, including Anthropic will by this time next year, have actually fallen under some degree of like, regulation in, in Europe.
- And as part of that, the EU and the AI office and the European Commission is gonna have to figure out what kind of testing and evaluation and everything else means - And, and, and yeah. Presumably working with governments, with their safety institutes and so on. - And, and also just let me just unspool a few other things.
- You know, there's the, the AI Safety Institute we've spoken about here. Yeah. But there's one being stood up in Canada. There's one being stood up in Japan. There's one being stood up in like many other countries. I'm not even sure I can name all of them because some of them are private and known only to me. - Right. - Yeah. But there's lots of them. - Right, - Right.
- So there's gonna be this network of government embassies to this new silicon nation state that's being built, being stood up around the world. And also China is increasingly raising the issue of AI at the UN because China feels that it's probably not as integrated into the international conversation as it could be. And so it's trying to use the UN as a venue to raise AI.
- And obviously we have the UN General Assembly happening this week while we're talking, we announced there the State Department that we're going to make Claude available to kind of people around the world in a, in a subsidized way. Many other companies did the same. There's just huge action happening internationally. And I think if people thought AI is kind of slowing down, the governments are all awake now.
- So next year, 2025, we're gonna see wild new, new things happening in policy. So I'm gonna be busy. - Yes, indeed. Indeed. So when you're talking to these policy makers, either at the Labor Party conference or in the US or in other countries, is there one thing that you tend to, to, to say, I mean, you're deploying the various metaphors and you're talking about how these systems work.
- What's the kind of one thing that you try and get across to, to everyone to really grab their attention on this issue? - I, I try to, I always say, you know what the, the leaders of these companies are saying what, you know, Dario and Sam and Demis are all saying when they talk about artificial general intelligence, it's not a marketing term, it's a general thing that they believe in.
- They believe that they are, have a chance of building a generally intelligent kind of synthetic intelligence with the, the creativity of a human, you know, if it runs at machine speed. And that is a completely world changing thing if any of us succeed.
- And I just try and leave them with like, yes, we have all of these problems around us today. We have to obviously think about how we test these systems, how we integrate from into the economy, but if we're right, really, really wild stuff is going to happen, but we'll demand a much larger scale policy response than any kind of normal technology has in the past.
- And I just try and impress upon them that that's a real thing that these people at these labs believe. - Yeah, this is not a normal situation that we're in. - Basically neither the, the, the upsides of the, the immense kind of economic abundance nor potentially the downsides like certain misuses or risks, neither of these will be normal.
- They'll be like abnormally sized in their effect and it's worth leading sort of making sure we understand that. - Jack, aside from signing up to your newsletter and, you know, keeping eye on the Anthropic site for research updates and so on, what, what do, what should people do to keep up with this? I mean, one of the, one of the things that I certainly notice that lots people notice is that it's just an endless amount of stuff on AI.
- Like, you can't, you can't focus on any other things in the world. Yeah. Because there's just so much AI news happening every day. New models, new uses, new weird things coming up. What, what, how do you go about trying to keep up with this stuff and then summarizing it for people? - One thing is just using the AI systems themselves and very early versions of Claude, I would sort of talk to and find vaguely useful, but also kind of like a curiosity. And I was like, huh, this AI stuff we're working on,
- it's kind of interesting. I can get it to do funny things. But now I actually just use it, I use it like a tool partly because I've learned how to get most outta it by talking to it a lot. But also that the technology has just become a lot more capable and a lot more useful.
- And so I think that people should try to use the AI tools that are available today. Many of them, including Claude, are available for free. You can just start using it. I would also recommend that people, I guess just try and reckon with the, the implications of a world where the only real limiter on what you can do as a person is probably going to become under optimistic versions of this scenario.
- Your own kind of creativity and how you use that. I mean, you and I both have, both have young kids and I've been thinking about this since we're gonna go to school at some point. Mostly I think, oh, you should, you should like use these tools, but you should mostly just like be incredibly creative. Because it's the creativity stuff that is going to allow you to kind of get the most out of it and, and use this in all the unexpected ways.
- Well, that's often the, the, the trouble people have is they, they they get, you know, the Claude app on their phone or whatever. Yeah. They go, they go to claude.ai and then they sit and go, okay, what, what do I do now? And the, as you say that the limit is, is you need to think of cool things to do. Yeah. You know, this can do so much stuff.
- And that I think, you know, we can, we can help by pointing people in the right direction. Yeah. We have lots of ideas for people and so on. But yeah, the real, the real, you know, amazing uses are gonna come from just incredibly creative humans thinking of cool things to do with these. - And one straightforward idea is just spend almost 10 years writing 350 short stories and then ask Claude what it thinks of them. That's like an easy thing you can get started with today.
- There you go. There's your way to start with AI. Exactly. I mean, who knows what the world might look like in 10 years after this. Jack, thank you so much for, for talking to me today. Oh, thanks very much. It's been a great pleasure. And you can find out more on anthropic.com, but also on Jack's Substack, which is what's the- - importai.substack.com.
- That's the one. Thank you so much for watching and I'll see you in the next one.