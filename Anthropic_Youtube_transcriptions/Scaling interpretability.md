# Scaling interpretability

URL: https://www.youtube.com/watch?v=sQar5NNGbw4
数値: 17
時間: 53m

- I'm Josh Batson, and I'm here with other members of the interpretability team at Anthropic to talk about some of the engineering work that went into our big recent release about interpreting the insides of Claude 3 Sonnet. So why don't we start with some introductions. Jonathan, who are you? - I'm Jonathan Marcus.
- I have worked on the interpretability team for a amazingly long, eight months. Prior to this, I worked at Jump trading, doing quantitative finance for like 13 years. - Great. Adly? - Yeah, my name is Adly. I am also on the interpretability team. I've been here doing exchange learning stuff and sparse auto-encoder stuff for about the last 14 months.
- Before this I was working on efficient, large lingual modeling inference at another startup - TC? - Yeah, and I'm Tom or TC. I've been on the interp team about the last year working on dictionary learning. Before that I worked at the same company Jonathan did. So jump doing high frequency trading and before that I was at Facebook for five years doing backend info work there.
- So the reason we're here now is because there was a big interpretability release recently. What were you trying to do there and why? - Yeah, I think that the best way to describe this is that back last year we published paper called Towards Monosemanticity, which really demonstrated that this technique could work to extract interpretable features on a very, very small language model.
- And then in the months since then, we've just been scaling this up until we reach the size of getting really good features from one of the models that is deployed into production by Anthropic. - Help me understand what's the difference between like a small language model and the one you were tackling for this work? - Yeah, the small one would be so different from any language model you've actually used.
- Like if you tried to ask it any sort of question that you might think a language model would be very good at, it's gonna totally fail everything. It's just a kind of a very, very poor model. So it was helpful for the early work we were doing because we think it has a lot of the same structure as a large model, but it's much smaller so it's much easier to work with, but it's not useful for any actual task.
- And even if you asked it a fairly basic question like what do cats say? I'm not confident it would actually get that right. - It wouldn't meow. - No, I don't think so. But maybe we didn't actually try. - Someone said a good analogy, which is like eight months ago, it's like we said, hey, I think the earth is made of dirt.
- And so we had a hand drill, and we went a couple inches down, we're like, hey, there's dirt there. And now we made this like giant laser drill and went into the earth's mantle like, hey, there's lava there. And yes, I know that person was you. (Josh laughs) - Somebody. - Somebody. And I think that's a really good that when you said that, it really stuck with me because it's like, yes, it's technically the same thing.
- And yes, we expected there to be lava, but it's just been a huge engineering effort to actually drill down, ha ha, and figure out what's down there. And we've actually found a lot of what we expected, but it's really cool now that we're there.
- I think the thing that I want to add there is just how rewarding it is to look at these large language models that can actually do all of these really powerful things. So in the one layer model we were finding features, but the features corresponded to things like counting to 10 and generating the random string of letters and numbers that you see after URLs.
- And when you scale this up to a much more powerful model, the same technique can find features that really just are interesting, not just from a scientific perspective, but that just represent interesting nuanced topics. And that can really just shine a light into how the system's able to perform really hard tasks and in particular large language models can perform tasks that we don't know how to program computers to do.
- They really just like have all of these capabilities that we don't understand. And so if you can find the features from them, then you can really get this really fascinating insight into how they are able to do these things. - What are some of the features that you saw that were the most like striking or moving to you? - I really liked the functions that add numbers in a code feature, and that it was kind of not very narrow and not just firing on functions that are kind of obviously A plus B, but if there's some function which calls
- some other function, which is adding, then the feature also lights up for that. So it has some like deeper understanding of what a function that adds is than the very basic one. I thought that was super cool, and maybe surprising that it exists, but for a lot of these features when you first see them, you're shocked and then when you think about it more you're like, oh yeah, that seems like a useful and very reasonable thing for the model to do, maybe I shouldn't have been so surprised that it was there. - I remember first finding the veganism feature
- that it was really cool, and I did not expect to see it. It was not even the biggest model, and I'm not a vegan, but, disclaimer, but it was really interesting to see that the same model could identify the concept of not eating meat, of being worried about factory farming and not wearing leather, but also in a lot of different languages.
- And the fact that I was able to tie all of these concepts together, I like couldn't believe what I was seeing. The model actually, it's not just repeating arbitrary words in some random way. It like definitely very concretely, this model, this connection was already built into the model, and we just discovered it in there, and that really blew my mind.
- Yeah, I think one of the things which was kind of impressive for me was getting a sense of like how the model's thinking about this stuff. I think when language models first started getting big there was some notion that maybe they're just repeating things they've seen in the training data and when it says something it's because there was an extremely similar sentence out there, and it just kind of grabbed that and then gave you whatever somebody said in that context.
- But seeing these features that are like multimodal, multilingual about something. - What do you mean multimodal? - Multimodal, I mean that the image of the thing makes the same feature fire as the text of the thing. One of my favorite ones there was, there was a feature about back doors in code that also fired on images of like hacked USB thumb drives and various forms of subterfuge.
- Yeah, yeah, I think there were like five or six different devices with like hidden cameras in them, and it fired for like various various hidden cameras in like everyday objects. Which again, in hindsight, like it makes total sense. But I definitely wouldn't have guessed that.
- Right, the part of the model that literally recognized like a line of code that had a problem would be the same thing, which is like there's a pen with a camera in it. - And then even you could like artificially activate that, and say, hey, can you finish my function? And it would write it while introducing a subtle vulnerability that could be hacked later. That one really blew my mind.
- I did appreciate that Claude was kind enough to label the function backdoor. (everybody laughs) Any other favorites? - Can we talk about Golden Gate Claude? - Yeah. - I mean Golden Gate Claude was so much fun. - What is Golden Gate Claude? - What is Golden Gate Claude? Exactly. Golden Gate Claude was, so we had a feature in the paper, it was kind of the headline and we really liked it where Claude would activate on descriptions of the Golden Gate bridge, the iconic, majestic towering bridge between Marin and San Francisco.
- So someone had the amazing idea of, hey, do you think we can talk to Golden Gate Claude? And this is one of my favorite parts about Anthropic. I thought this was gonna be hard, but then someone from the engineering team just went into our code base, figured it out, and implemented Golden Gate Claude as an experiment.
- Like, hey, do you think I can actually take the results of your dictionary learning paper and just use it? And then we tried it, and it worked, and then everybody started playing with it. And that was such a cool experience to actually have the results of our paper brought to life like that. - Yeah, it was incredible.
- We put out the paper on Tuesday morning, and then we were all out to dinner on Tuesday night. And people inside the company were excited by that figure where when we turned on the Golden Gate feature-- - That's right. - And asked Claude, what is your physical form? Claude said that it was the Majestic Golden Gate bridge itself, and that was just like a little static thing. And Oliver was like, let's do it.
- Let's make Golden Gate Claude. And while we were eating and celebrating, they started working and 36 hours later-- - We checked it. - There was a polished product that could be shipped and the world got to like talk to a model that had this feature we discovered only weeks before amplified. And getting a feeling for what it means to kind of drive the model in one direction or another.
- So I know the model was really big, Claude's really big compared to the ones we were working on before. What did you have to do to take the dictionary learning technique for finding the features and scale it up to work on something like that? - Oh man.
- Yeah, first of all, I just wanna say, this is a really long question to answer because this was probably the bulk of our work between the publishing of Towards Monosemanticity and the publishing of this paper. Like there were just so many things, and we should get into them, but I just want to emphasize that we are just not going to be able to talk about a fraction of the things that had to be done here because this was just such a really big effort.
- I also wanna like frame it slightly differently. Like when when we first got the results in Towards Monosemanticity, and we started thinking about what we're doing next, we didn't immediately go, oh, we're gonna scale this to Sonnet, this will definitely work. We didn't know if this would work at like larger scale.
- So we didn't wanna spend eight months just scaling and then check if it actually worked. So it was much more of a kind of back and forth between the engineering side and the research side to kinda what experiments can we do to scale this up to give us more confidence that scaling it up actually works.
- So it was kind of not this like monolithic thing that we could just plan, it was something where we were kind of scaling it in pieces, confirming that this still looks good. A then scaling it more as we got more confidence. - So what does scaling actually look like? - So I think one example here is very illustrative, this is something that came up pretty early on in the process as we were scaling this up where when we were working on Towards Monosemanticity, all of our models fit on a single GPU.
- Every sparse auto encoder that we trained fit on a single GPU. And what we realized very quickly is if you're going to keep scaling this up, this is no longer going to work. You are going to have to chain a bunch of GPUs together and implement something that we call OP charting where you take the parameters of the sparse auto encoder and distribute them among a large number of GPUs.
- Can I interrupt, what is a sparse auto encoder? Maybe not everyone knows what they are. - So an auto encoder is something that takes in some data, some vector and transforms it into another representation from which you can read back The original. The auto is that you get back the original encoder if you change the representation.
- Sparse auto encoder is one where this new representation you get is very sparse. There's only a few elements in there that are non-zero. And this can be really nice because if you're trying to understand what does this data mean, and there's exactly three non-zero elements in the encoding, you can just go look at each of those.
- Whereas if the original vector had a thousand components or something, it might not make any sense. And the basic bet we made here, which is shocking to me that this worked at all, was we took some of the latent states, the vectors inside Claude, trained a sparse auto encoder to see if we could represent those as a sum of just a few pieces each time.
- And the answer was yes. And then when we looked at each of those pieces, they were shockingly interpretable. From a math perspective, I think a sparse auto encoder's really simple. There's just two matrices involved. From an engineering perspective, I think it proved to be a lot less simple.
- And so you could write down the math from our paper in October, and we could copy and paste basically the same math to our paper now. And that is not how it worked like on the silicon. - Oh my God. - I think something that feels really interesting here to me is that when we first started this project last year, we were experimenting with a bunch of different techniques for this, and we were experimenting with a bunch of more complicated techniques. There's a lot of fancy math out there that addresses this problem.
- It's possible that that math might still work better, but we really just saw a lot of success with sparse auto encoders because you could just really scale them up. We tried running all of these other techniques, but you could only run them on a small amount of data.
- And one of the things we realized is that in order to see really good results, you have to run this on a lot of data, much more data than anybody in the classic mathematical literature ever does. And so sparse auto encoders are in some way just mindbogglingly simple in a pretty beautiful way.
- And it's just this philosophy that if you take a simple algorithm, and it's scalable, and you can just really turn up the numbers, you can get really beautiful stuff out of it. - But turning the numbers up, like that's the hard part. I do almost no math. (laughs) You do math, you do math, but like so much goes into, hey, let's make this thing 10, 100, 1,000 X and so on bigger, and that just breaks all our abstractions, breaks all our code in so many different ways.
- It just becomes too big in all these dimensions that you were totally unprepared for and causes weird bugs. - I think one of you told me one of those dimensions was around like shuffling the data. - Oh yeah. - Someone talked through what the shuffle problem was and what you had to do. - Yeah, so there's this perennially hard problem in machine learning where you have your input data, and you want to make sure that if you've got like a whole bunch of A's and a whole bunch of B's and a whole bunch of C's and a whole bunch of D's, and you feed them through your model, if you just feed them in an order,
- it's gonna learn, hey, I should only learn A's, hey, I should only learn B's. But if it's all mixed up, then it has to learn the whole distribution at every step. And the shuffle process is very easy. When your data is small, you load into memory, do like random shuffle, and then you write it back out.
- That's not that hard. Now you ha what do you do when you have petabytes of data? It's like, oh, oh, so. - I guess it might be like if you have to shuffle a deck of cards, you can just do it with your hands. - Yeah. - And if somebody gave you seven consecutive miles of cards stacked to end to end, it's like not clear how you would shuffle that deck.
- Yeah actually. - At all. - That's a really good analogy. So it's like I have a hundred warehouses full of cards. - Yeah. - And I ended up like, we talked about a lot figured out, hey, is there a way to do this in parallel? And it's like, well if you're gonna shuffle a hundred warehouses of cards, first, you're gonna shuffle one warehouse of cards and you'll break it up into a hundred sub-problems.
- And then how do I shuffle one warehouse of cards? I'm gonna break it up and like do it by section, and then you're gonna like mix the different sections in some provable way that makes sure that every section gets mixed with every other section. And like, I don't know, that sounds really simple and in a sense once we had understood the problem that, oh, we just need to make this like multi-stage parallel shuffle where we break it down like, oh, probably anyone could, not anyone, but like a lot of people can like implement that as part of like a coding interview. It's like not that hard of an algorithmic problem,
- but to get to the point where we even realized like, oh, just framing the problem was 90% of the work, once we did that, and we could conceptualize it as, oh, we need to do a multi-stage parallel shuffle. And then once you get your like recursion properly defined, then it's a pretty easy task to scale it to something, oh, you wanna do a hundred terabytes, you wanna do 10 petabytes? Cool, just add another layer.
- I think some more context there is interesting because we're gonna focusing on like the part after we decided we're gonna speed up shuffle, but I think the part before that kind of shows a lot of what this job is about, where what was happening is we were scaling things up, and then running experiments as we scale. And the shuffle step before we made it better was taking longer and longer.
- So we knew that like this step is not scaling well, and it's making it slower to get research results, but also we know there's something better there. But there wasn't something that I could do in like a couple hours. I was like, oh, this is maybe a few days, a few weeks.
- So we're kind of putting it off because we can still get experimental results until eventually this thing's taking 24 hours, something like that, and we're like, okay, we finally need to like fix that. And then I think the fix that we did, you kind of could do something that is maybe like more like perfect or totally nail shuffle, but we're not focused on like what is the like optimal, the platonic ideal of parallel shuffle.
- What we care is our job's taking 24 hours, how can it not take 24 hours? So I think a lot of this job is like, we want to get experimental results, that's our focus. And then given that goal, how do you get those? So it's generally not how do you make any step perfect.
- It's how do you make any step as good as we need to to get the results that we need right now? And then as those results come back, you gain more or less confidence in the approach which you have. And as you have more confidence that this code base and this approach is something we're gonna be using six months later, a year later, you're willing to invest more time into making things better.
- And I think the kind of the heart of the job is how do you make that trade off of how much time to invest into any one piece of this whole pipeline. - Yeah, I want to draw that out more. It sounds like this kind of engineering where there's an experimental result at the end feels like a different process than maybe producing a product.
- Like could you say more about what it's like to do engineering for research? - Yeah, I think it's interesting to compare it to my first job at Facebook, and I was building a service, a backend service, which like powered the Facebook website. And I would say the big difference is kind of the requirements of the code at that job at Facebook, it never really changed. We always knew that this was going to run at scale. It like couldn't crash.
- We cared about the cost of the servers we were running on, but I was there a few years, and it was always the same goals, where in a kind of research engineering job now, like you don't know which bits of the code you're gonna be throwing out in two weeks and then which bits of the code you're gonna be using like years later.
- And a lot of the like original dictionary learning code was methods which we've like deleted, they're gone. We're like never touching it. And spending time like making that code perfect would be totally wasted because it's deleted, but also like over this year long process, we've kind of honed in on what we're doing is working and this core thing is good, we need to make this better, we're gonna be using this longer and if the code quality is crap, like it's gonna be slowing us down for like years, so we need to to like really go and polish this more. So I think you kind of constantly have
- to keep those trade offs in the back of your head, and they're kind of changing under you as you work. - There's another dimension to this that I'd like to talk more about, which is there's a whole bunch of ideas that we want to try.
- And when you're looking at implementing these ideas, you're thinking about how to design the infrastructure and like with any software design, certain infrastructure designs are gonna make certain things easy and certain things hard. So there's this really tricky thing, and I think in some ways it's an impossible problem, and you can only try to do this very poorly, but trying to anticipate which directions you want to go in in the future, trying to anticipate what general categories of ideas you might want to try and try to anticipate how do we make these general categories easier and what are we closing off, what are we making harder to do?
- And trying to make those trade-offs is a really, really difficult challenge that we try our best at, but is something that is just impossible to be perfect at. - Did you make any big mistakes? - None. (everybody laughs) I think a lot of the errors here feel more like we maybe should have cleaned something up a month sooner.
- So it's like, oh, maybe we should have done this sooner, because your trade offs are changing under you. If right now you're like, I'm not really sure, should we do this, should we not do this? In a month, it's frequently blindingly obvious like, oh yeah, we should definitely do that. So you do lose that month of like, it would've been better if you got there sooner.
- But I generally think you kind of get shoved in the right direction eventually. - Bu I also think that there's an important point that I am not a professional scientist where I'm just looking to publish papers. I'm also not a professional engineer where I'm just looking to build the most perfect, beautiful, harmonious, well attractive system.
- Like we have a specific target, which is being able to figure out and do enough science to figure out interpretability, so that we know how these machines work to achieve a specific safety goal. That we have to do enough science to get there. We have to build enough engineering stuff to support the science, but ultimately, it's quite possible at the end of the day, we will throw away every single thing we've built except for that one end result.
- And so I don't wanna spend any additional time researching stuff that's not gonna help. I don't wanna spend any additional time building stuff that's not gonna help and like getting that trade off is super hard. I'm not always so good at it, but you guys are so thank you. (Adly laughs) (TC laughs) - It's also always easier in hindsight.
- Yeah. (laughs) So what was the most confounding bug in this process? - Yeah, so one of the really dangerous parts of machine learning, and especially when you join machine learning on this weird undiscovered topic is that it's really hard to know if you've written your code right. I remember my machine learning professor told me this in college, and I'm like, that doesn't seem so hard.
- This like, can't possibly be such a big problem. And then you realize that this is just a thing that is going to eat up more of your time than any other problem. So we had cases where we just lost weeks of effort because we had something, and we had some evaluation metrics and the evaluation metrics are bugged in a way that makes them too good to be believed and really exciting.
- And we spend a lot of time chasing that down before we realize that there's just some really subtle bug in our metrics, and it's very hard to test for that. And you basically end up needing to spend a lot of engineering time trying to make sure that these things work, and that you can trust your evaluations here.
- Bugs and metrics are scary because if you're trying to make the number go down, and the number's going down, and you're like, this is great, everything's great. And then it turns out you were just like chasing a complete illusion for weeks. So how do you deal with that? Like what is it test, like what's testing like, for kind of research code? - Yeah, I think kind of correctness bugs like that are very difficult to test because it's kind of not clear what the correct answer is. So your like, classic unit test doesn't really cover this well.
- I think the thing that helped here was to log as many metrics as you possibly can while this process is training, can you think of every possible number you can like log and then graph those, and then for your runs you can stare at these graphs and be like, what should this look like? Does this make sense? And I think there's no easy answer here, it's just time.
- I think the other piece of this is just really going through the code carefully and being like, I know what the math for the ML says this should be doing, but like what is it actually doing? And we've had a number of times where that didn't match. And I think tracking those down is a very important thing.
- And I would also say that that like there's latent bugs in master that you're worried about. I think there's like another way that this comes up of every time you have a new idea for how the ML will change, you code that up, and you run it, and then sometimes the results are like, oh, this is worse than than your baseline, and you're not really sure, was the idea bad? Or when I coded it up, was it bugged? And you don't know.
- And I think that's kind of a difficult trade off of what to do next because you can go, and you can stare at the code, you can go and stare at graphs and try and understand like was it bugged in some ways? But at some point you have to decide this idea doesn't work, and I'm giving up and I'm moving on to like something else.
- One of the striking things for me who has more of a science background working on a team of really skilled engineers has been realizing the power of like pulling some of the engineering work forward to increase your iteration time. And I think that the more that your ideas matter, then you wanna spend a lot of time thinking, but if you have no idea of what's gonna work or not, then making it so you can test a lot of ideas quickly really pays off.
- And this kind of relentless looking at how would I run this experiment? Okay, could I run that experiment in a day instead of in a week? Could I run it in an hour instead of in a day? Could I kick it off in a minute? And like your ideas might be better, but like no one has ideas that are like 200 times better, such that you would rather take that long to to run an experiment, - Speak for yourself. (everybody laughs) - I think this comes back to the short term versus long term trade off, which is I think really
- just like one of the fundamental tensions about doing this sort of research engineering where you have to decide how much effort to invest into making things better long term versus how much you want to just try something, try it in the hackiest possible way and get results quickly.
- And I think that unlike in a lot of traditional engineering, you don't just want to lean all the way towards the long term thing. It depends on a lot of factors, it depends on how confident you are that something in this general area will work. It depends on how reusable do you think this infrastructure is going to be in the future and how easy is this going to be to code up and get working really well.
- But it's also informed by the science of do we think that dictionary learning is a process that we should be going so all in on? Is having the faith guided by our scientific intuition that if we keep pushing here, we're pushing blindly. Like we don't actually know if we're gonna be going towards anywhere until we drill down far enough and oh, there's lava, like it's just a lot of dirt.
- And then all of a sudden you, you pull back up, and you realize, oh my gosh, we've actually gone so far, and we've actually found something. But for a while you're just fumbling in the dark and like nothing works, nothing looks good, nothing makes sense, but you just have to believe that if we keep researching in this direction, maybe there's signs of life, and eventually we're gonna see something useful.
- Personal question, why do you like doing this work? - So for me, in my previous roles at the company I used to work on the inference team, so the inference team is, there is much less of the research aspect of it. We kind of know exactly the operations that need to be done, the like math, and we just need to make them go really, really fast. And it leads you to these really interesting systems.
- Low level GPU optimization problems. But to me it's like I can kind of plan out what the next six months will look like. You can kind of figure out, we're gonna design it exactly like this, and we need to do A, B, C, and D and you have this like exact plan and then you go and do it. And I kind of found the work of doing that exact plan a little tedious or boring.
- There are plenty of people at the company who love that. I just don't personally, where on this team, like we can't plan six months out, right? And we don't know what to actually build, and you're following where the research results lead you and kind of everything's constantly changing. So I really love that piece of this job.
- Adly, what do you like about this work? - Yeah, I think there's two questions there, which is why I love the research part and why I love the engineering part because really I love both of them, and I love the research part just 'cause, honestly, there's no better way to describe than this is just a really beautiful problem, and it's really fascinating to try to understand this, and it feels amazing when you can shine a tiny little bit of light in the black box of models. - One of the things that I like about this is
- the engineering is a lot of fun for sure, but it's also the problem itself. It's like, and this goes back to why do I like, how does this compare to my previous job doing quant finance versus this? Studying markets was actually very fascinating. They're always changing. There was a lot of interesting modeling to be done, but here we're essentially doing like computational neuroscience on an artificial mind, and no one's ever done that before in history because these things have never existed, and we're like among the first people right now
- to ever have access to artificial minds this big with the amount of computational infrastructure that it takes to analyze them. We're literally like trying to figure out how these things think. We're studying cognition in a very quantitative way and it's so mind blowing to me that almost the same skillset that I was previously using to predict the next price now becomes decoding thought.
- And I loved finance for many, many years, but this just feels so much more meaningful to me. - And I think the really exciting part about trying to tackle these problems with engineering is that it makes them solvable. If you ask yourself how do you neuroscience in an artificial mind, that's not the type of problem that you're really like going to solve or maybe you could solve it, but you don't have a high concept of anything.
- There is something about building the infrastructure to do this and building the infrastructure to do a lot of experiments that makes it feel possible to say we are actually going to do this. Engineering is just a way of making this successful and making this possible. - So for the people listening to us who think this sounds kind of cool, do you have any advice about getting into interpretability research or AI research from an engineering side? - The first thing I'd say is I think a lot of people think the work of the interpretability team needs much more of the research skillset than it actually does, like the research skillset is important, but the engineering skillset
- really, really matters too. So we are not just looking at people who have only done like math and ML, we need people who are very strong at coding too. And like currently we're bottlenecked by hiring very, very strong engineers. So we need more people like that kind of asking us for jobs would be the first thing.
- What you can do if you're interested in this, and you're a great engineer is ask us for a job. (everybody laughs) because we are hiring people like you. - That's silly, but I think it's very easy to underestimate the contributions that you're able to make, especially if you think of yourself more as an engineer coming into this. And I really the advice is just to apply.
- The other thing I would note on the engineering skill sets, what we're looking for, what people might learn is that I think we need a lot of breadth. We are not like we need to make GPUs go fast for the work that we do, but we're not pushing things to the bleeding edge, right? So we need people who can kind of do a bunch of different skills and come in and notice like, oh, I can do a quick change, which gives us a big win.
- We aren't really looking for the skillset of, I can spend two months to use the graphics card 10% more efficiently here, we're not gonna spend two months on that. We're gonna move on to parallelizing other jobs, figuring out why some Python code's really slow. So you kinda need this breadth to be able to figure out which point in this complicated pipeline is the bottleneck right now, and let's go make that like a bit better in a few days is kind of a big skill that we'd really love to see more of.
- Yeah, it seems like the team has a lot of full stack engineering where the stack goes down to like, you could do fuse GPU kernels and all the way up to building frontend interfaces for like looking at how images make Claude talk differently, and that you never know where in that entire chain might be the thing you need to do.
- There was I remember, a frontend bug the other day that actually turned out to be like an OP charting bug, so you thought this might be okay, the server is rejecting your request? And then it just turned out that no actually we had shuffled around these tensors in a transposed way, and that needed to be what's fixed.
- And so it actually means there's a ton of ways to contribute and also this kind of breadth and fluency can really pay off. - So Josh, you're a scientist much more than us, I'd say we all shade pretty on the engineering side. What's your biggest frustration with people like me? (everybody laughs) - I mean people like you are so charming.
- I don't think there's a frustration, I think it makes for very good collaborations because oftentimes we're so early days that there's often a lot of room for improvement and sometimes it turns out that like we should just be like plotting the correct metric or changing the initialization scheme for a matrix that could also speed up the training process by five X, and it could be that you need to speed up the training process by five X by parallelization.
- And so I think that there's just these opportunities, this is what I mean by the full stack actually continues all the way into like the mathematics and all of these pieces of it. So I think that it's really helpful to have a very interdisciplinary approach to this stuff because sometimes you can sharpen, like did you really need to run your ablations over the entire data set, or are you trying to estimate a scaler? At which point statistics tells you need a thousand samples, and then you're like pretty much good, and you can save a lot of time. - I think also I've actually really enjoyed,
- even though you're on like separate sub-teams, so I don't get to work with you nearly enough, I really enjoyed the few times that we did get to collaborate. 'cause I think we have such complimentary skill sets where. I've said it before, I'm not that great at the math. I still don't know ML, sorry guys, I'll leave.
- (everybody laughs) But like I really like the culture of collaboration where you and I will just sit together and pair a program on a problem, and we have very complimentary interests and skills where when we work together we are just like very, very powerful. And I think that that's a lesson, the reason I bring this up is for people considering, hey, do you think I could come in to interpret and be useful? It's like, yes, if you are good at some of these things, but not all, there's so much value when you pair with other people who have different skill sets, and we really benefit from that collaboration. - I think that one of the really fun things about this is
- you start to learn from those collaborations like the shape of a problem that could be solved, which is like well in advance of having any idea of how to solve it, but I'm like, hmm, like I bet Jonathan could help with like this part of the thing feel stuck, and I don't know enough to yet be able to do that.
- But then we can sit together and like, oh yeah, that's the kind of thing that I could bang out right now, or on the visualization side, I feel like I'm clicking around between 17 windows right now, and actually, we've gotten the parallelization down, it's like super fast to run these jobs and now it's taking me like 30 minutes to like look at the results, and then we bring in Pierce who's like, "Oh, yeah, yeah, yeah, yeah, we can totally make that part better.
- " And then when you put that all together you get this like really incredible like scientific system where actually all of the parts sort of work and what comes out the other side is some of the more beautiful papers I think I've ever been involved in. Or actually got me in part to join the team was just like, you just see these like jewel-like figures that come from, people obsessed with like working in Figma to just like dial all the details in, which is not something that I thought maybe working in Figma isn't part of the standard like engineering toolkit, but it turns out that like that also is a force multiplier.
- One like explicit thing that I wanna mention that was kind of baked into those answers is like how the team is structured here. I think a lot of people think that there's a separate research team and a separate engineering team, and kind of throughout the conversation here we've been talking about the interplay between those. So like separating those like just doesn't work.
- Like we don't do that. There isn't like these separate researchers who are telling the engineers, like build this. These problems are fundamentally entwined together, and you have to work on them together. So the way the like whole company works, not just the interpretability team is kind of the research and the engineering always goes together, and that's just absolutely crucial for this job.
- Adly, if a friend came to you and was like, what was the most fun or weird or quirky thing you got to do? - Yeah, I think that there is like a surprising collection of problems that comes in after you have trained 34 million features, and now you want to, as silly as it sounds, you want to see what these features do.
- And this is a tricky problem at scale because these features only activate on very specific sequences of text, that's what the sparse on sparse auto encoder means. And so if you want to really visualize all of them, you have to run a lot of features through a lot of text and then do things like, we also want to visualize what does this feature do on the nearby text and what does the distribution of this feature look like? And solve a bunch of problems like that, that I believe at this point it's something like a 10 or 12 step very distributed pipeline
- just because this is one of the things that breaks really quickly once you scale up the problem. And there's just so many steps that something is always breaking and something different is always becoming the bottleneck. And so it's this process of just looking at this, finding the bottleneck and trying to distribute that further.
- Yeah, sometimes things like even matrix multiplication doesn't work anymore where you realize that you wanna understand interactions. This is on my team between 34 million features here and 34 million features there. And genuinely you could just multiply the matrices, but then you couldn't store the result anywhere or put the result anywhere. And so you're starting to do some like fancy looping indexing and compression to compute a product.
- Just big numbers times big numbers are very big numbers. - One of the things which we hit is the default PyTorch-MAML implementation for certain shape matrix multiplies is just much slower. So we're like profiling jobs, and we look at it and most of our time is in matrix multiply. So we think this is great, we're running really fast, but we calculate efficiency numbers, efficiency's not great.
- So we then go to someone else else at the company who's kind of more of an expert in this narrow area, and he tells us that, oh yeah, try this other matrix multiply implementation, it'll be much faster. And we're generally doing that of like when we get to the really thorny problems like that, we just ask someone else at the company because we're not experts at that, but it does matter, and we do need to make these things faster.
- So we were using the unbeknownst to us, like a slow version of multiply these matrices. - Well it is a version that is normally fast, but for the specific shapes of the tensors we were running matrix multiply on, it was not fast. And there's kind of different implementations for that. So under the hood for matrix multiply, what generally happens is based on the shapes of the matrices, there's like different ways, the like GPU kernels actually work.
- So some implementations kind of pick the wrong approach and are just randomly slower. So we kind of run into problems like that. It's randomly slower, how do we fix this? And yeah, you kind of don't have the time to go be an expert in this area. You just need to kind of quickly find something that'll speed it up.
- I think this is such a fun example because you would think that matrix multiplication is just heavily optimized, but in a very physical sense our problem was just a weird shape. It was a weirdly-shaped matrix, and so we just run into all of these problems because interpretability research is just doing really weird things like this. And so you run into all of these weird things that happen.
- Yeah, thinking like distributed is of funny for this too. We were doing this in like attribution calculations where you're just multiplying a vector by a bunch of other vectors and like you have to think carefully about where they are living and which direction you send information 'cause if you send this over here, you get to send some scalers back.
- But if you send this over here, it's like a matrix is going back and all of a sudden like you've spent just enormous amounts of time shuttling data back and forth where like again, I was trained as a mathematician, you write the equation, and all of the letters are on the same line, right? There is no like communication bottleneck between the A and the V that it's next to.
- Yeah, I was looking at a open source implementation of a sparse auto encoder training that only runs on a single graphics card. And I was just shocked by like, this is so simple, like this is so easy, why do we have so much code? And then you go through all the various points where we had to like scale this up a thousand times bigger, and it's like that is where all this code comes from, and there's kind of so many little battles there of like this random thing doesn't scale, is like two X slower, that like we've put in,
- which we didn't have to do back when when like we were doing very, very small jobs which just fit on a single graphics card. - I think that also speaks some to some of the complementarity of the work that can kind of happen in academia or more open source environments and what you can do at a company with the scaled models where like you can try out a lot of ideas at small scale, and it like isn't that hard from an engineering perspective.
- And then to get that to actually work on models that are many orders of magnitude larger, you're just like entering new realms of physical difficulty to get anything off the ground. Sometimes it feels like there's the gift though, which is that in the bitter lesson that Richard Sutton talks about, which is sometimes the scalable thing is better because you can always put more scale in if you do the engineering, and you hit the upper limit of being clever. And so even though some of these methods are quite conceptually simple, it's turned out that like on the rich data distributions
- that actually make up these networks, they show really amazing things. - It's really fun. I think that the bitter lesson applies not just to training a model, but also to interpretability where I think people often think of interpretability as trying to get this like very principled understanding, and there is some of that, but there is a lot of that that just really has the same properties as the bitter lesson where you just take something simple and do it at scale, and you pick the scalable thing.
- And it is really beautiful to me that that works not just for making good models, but also for understanding models. - The other point I'd make with like scaling and the bitter lesson is that the company has given us access to the compute, which we need to actually scale this. And it's been really fun that like the thing blocking us from scaling further is like whether the ML actually works at that scale or the infrastructure works at that scale, it like hasn't been, can we actually get the graphics cards
- to like run on, which would be kind of a much more frustrating reason to not be able to scale. - Where do you see interpretability in a year? - I think that where I see interpretability in a year is if everything goes well. I mean this is a super bullish case but we will figure it. So we did one slice through the middle layer of Sonnet.
- And I would want to analyze the entirety of every layer, every piece of all of our production models. And not just analyze them, right now we only found features. We don't know how they fit together. We don't know how they work in a variety of different contexts, And I really want us to do the circuits work to figure out like what do these features mean on their own, what do they mean together working in concert? - Yeah, one thing that I think I'm just surprisingly excited about is just actually continuing to scale this up. There is a lot about what we need to do
- that is going to need to be different. There is definitely gonna be lots of opportunities to change the way we do things, but at the same time these things seem to work better as you keep scaling them up. And so I'm really excited about just trying to eke out the last few orders of magnitude and see what happens.
- And if you would like to help us with that, we are hiring, we would love to work with you. - Can I just say I love the phrase the last few orders of magnitude. (Adly laughs) There's so much in those few words. - S why are we doing interpretability? - I think one of the things I want to emphasize here is I have a lot of uncertainty about the types of safety challenges that are going to rise with large language models.
- And I'm very uncertain about the direction things will go in the future, but interpretability feels very robust to me. I'm very excited to work on this 'cause I think it can help with a really wide range of problems and a really wide range of scenarios. It's just understanding models seems good, and if you can do that better, that's probably helpful.
- Yeah, understanding models seems good, and if you can do that, it seems like it'll help you with any of the behaviors you might. And maybe that's something I really like about interpretability or rather the approaches we're taking which sort of completionist, right? It's trying to map the full diversity of the model because if you can do that, you can zoom in to the parts that you need later.
- Whereas if you're just focused on like one particular behavior of interest, it might not generalize, or it might be missing the important part of the story. And so you can do interpretability focused on like one behavior at a time, but if you want the whole picture, you need to scale. And that's why you need people like the ones at the table who can make the scaling happen. - Here, here, - Here, here. All right, hands in. And do 1, 2, 3, Claude.
- [Everybody] 1, 2, 3, Claude! (everybody laughs)